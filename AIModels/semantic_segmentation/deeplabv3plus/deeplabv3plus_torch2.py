# -*- coding: utf-8 -*-
"""DeepLabv3plus_normalized0to1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17qCHgOV7M-0_gzwE4RPJ2NC361KbiG5j
"""

# -------- Library --------
# Machine Learning
import tensorflow as tf
import torch
import torch.nn as nn
from torch import Tensor
import segmentation_models_pytorch as smp
from torchsummary import summary

# Data
import rasterio as rio
import csv
from PIL import Image
from torchvision.transforms import ToTensor
import pandas as pd
from mpl_toolkits.axes_grid1 import make_axes_locatable
from tifffile import imread
import numpy as np
import gc
import imghdr
from torchvision.datasets.vision import VisionDataset
from torch.utils.data import DataLoader
from torch.utils.data.dataset import Dataset
from torchvision import transforms
import imgaug.augmenters as iaa

# Graph
from typing import Any, Callable, Optional
from tqdm import tqdm
import matplotlib.pyplot as plt

# Directory manager
import os
import glob
from copy import deepcopy
from pathlib import Path
import copy
import time

# Metrics
from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score

# -------- Set up --------
# Classes
lista_class = {'building': 0, 'cultivated_area': 1, 'forest': 2, 'non_observed': 3, 'other_uses': 4,
               'pasture': 5, 'savanna_formation': 6, 'water': 7}

lista_class_r = {value: key for key, value in zip(lista_class.keys(), lista_class.values())}

# Constants
IMG_HEIGHT = 256
IMG_WIDTH = 256
NUM_CLASSES = 8
BATCH_SIZE = 2
EPOCHS = 40
PATIENCE = int(EPOCHS * (10 / 100))
exp_directory = '/Users/mateus.miranda/INPE-CAP/MSc/ai4luc/AIModels/trained_models/trabalho/deeplabv3plus/'

# Transform data to Tensor
# dataTransform = transforms.Compose([transforms.ToTensor()])

# GPU config
#torch.backends.cudnn.determinstic = True
#torch.backends.cudnn.benchmark = False

# Fetching the device that will be used throughout this notebook
#device = torch.device("cpu") if not torch.cuda.is_available() else torch.device("cuda:0")
device = torch.device('mps')
print("Using device", device)


# -------- Metrics --------
def compute_iou(label, pred):
    unique_labels = np.unique(label)
    num_unique_labels = len(unique_labels)
    iou = {}

    for index, val in enumerate(unique_labels):
        pred_i = pred == val
        label_i = label == val

        I = float((label_i & pred_i).sum())
        U = float((label_i | pred_i).sum())
        iou[lista_class_r[val]] = (I / U)

    return iou


def normalize0to1(imgs):
    norm_img = []
    for img in imgs:
        # Transpose

        # Normalize
        max_img = np.max(img)
        min_img = np.min(img)
        normalized_img = (img - min_img) / (max_img - min_img)

        # Crop
        cropcenter = iaa.CenterCropToFixedSize(height=256, width=256)
        cropped_raster = cropcenter.augment_image(normalized_img)

        norm_img.append(cropped_raster)

    return norm_img


# ------------- Load data -------------
# Load images
def load_patches(path):
    # Create the sequence of the images path sat[0]
    image_path = path + "/images/**/*.tif"
    # Take full path
    image_names = glob.glob(image_path)
    # File sort
    image_names.sort()
    image_names_split = image_names
    print(len(image_names_split), 'Images to train')

    # Create the sequence of the mask path sat[1]
    label_path = path + "/masks_reference/**/*.tif"
    # Take full path
    label_names = glob.glob(label_path)
    # File sort
    label_names.sort()
    label_names_split = label_names
    print(len(label_names_split), 'Masks to train')

    # lists
    image_patches = []
    label_patches = []

    # Reading images path
    for name in image_names_split:
        with rio.open(name) as raster:
            bands = []
            for i in range(1, 4 + 1):
                band = raster.read(i)
                # Crop
                cropcenter = iaa.CenterCropToFixedSize(height=256, width=256)
                cropped_raster = cropcenter.augment_image(band)
                bands.append(cropped_raster)
            image_patches.append(bands)

    # Normalizing 0 to 1
    imagenor_patches = normalize0to1(image_patches)

    # Reading labels path
    for name in label_names_split:
        with rio.open(name) as raster:
            # Read label
            label = raster.read(1)
            # Crop
            cropcenter = iaa.CenterCropToFixedSize(height=256, width=256)
            cropped_raster = cropcenter.augment_image(label)
            label_patches.append(cropped_raster)

    label_patches = np.asarray(label_patches).astype('int32')

    # To tensor
    imagenor_patches = torch.tensor(imagenor_patches, dtype=torch.float32)
    label_patches = torch.tensor(label_patches, dtype=torch.int32)

    return imagenor_patches, label_patches


# Load images and labels together in only on dataset
class SegmentationData(Dataset):
    def __init__(self, image_patches, label_patches):
        self.image_patches = image_patches
        self.label_patches = label_patches

    def __len__(self):
        return len(self.label_patches)

    def __getitem__(self, index):
        return self.image_patches[index], self.label_patches[index]


# --------------- Model ---------------

# DeepLabv3plus
def deeplabv3plus_model(num_channel: int, num_classes: int):
    deeplabv3plus = smp.DeepLabV3Plus(encoder_name='resnet50', encoder_weights=None,
                                      in_channels=num_channel, classes=num_classes, activation='softmax')

    return deeplabv3plus


# Training
def train_model(model, criterion, dataloaders, optimizer, metrics, save_path, num_epochs, name_dataset):
    global loss, batchsummary
    start = time.time()
    best_model_wts = deepcopy(model.state_dict())
    best_loss = 1e10

    # Use gpu if available
    model.to(device)

    # Initialize the log file for training and testing loss and metrics
    fieldnames = ['epoch', 'train_loss'] + \
                 [f'Train_{m}' for m in metrics.keys()]

    # Save the log in csv file
    with open(os.path.join(save_path, name_dataset + '_run1_DeepLabv3plus.csv'), 'w') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

    for epoch in range(1, num_epochs + 1):
        print('Epoch {}/{}'.format(epoch, num_epochs))
        print('-' * 10)

        batchsummary = {a: [np.nan] for a in fieldnames}
        batchsummary['Train_iou'] = {a: [np.nan] for a in lista_class_r.values()}

        model.train()

        for image_patch, label_patch in tqdm(iter(dataloaders)):
            # Normalizing data

            inputs = image_patch.to(device)
            masks = label_patch.to(device)

            # zero the parameter gradients
            optimizer.zero_grad()

            # Take the trained_models predictions
            outputs = model(inputs)

            # Create a valid target tensor for nn.CrossEntropyLoss
            masks = torch.squeeze(masks).type(torch.long)
            loss = criterion(outputs, masks)

            # Get class indices predicted
            y_pred = torch.argmax(outputs, dim=1).data.cpu().numpy().ravel()
            y_true = masks.data.cpu().numpy().ravel()

            for name, metric in metrics.items():
                if name == 'f1_score_weighted':
                    batchsummary['Train_' + name].append(
                        metric(y_true, y_pred, average='weighted'))
                elif name == 'iou':
                    iou = metric(y_true.astype('uint8'), y_pred)
                    for key, value in iou.items():
                        batchsummary['Train_' + name][key].append(value)
                else:
                    batchsummary['Train_' + name].append(
                        metric(y_true.astype('uint8'), y_pred))

            loss.backward()
            optimizer.step()
        batchsummary['epoch'] = epoch
        epoch_loss = loss
        batchsummary[f'train_loss'] = epoch_loss.item()
        print('Loss: {:.4}'.format(loss))

    model.eval()
    with open(os.path.join(save_path, name_dataset + '_run1_DeepLabv3plus.csv'), 'a') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writerow(batchsummary)
    if loss < best_loss:
        best_loss = loss
        best_model_wts = deepcopy(model.state_dict())

    time_elapsed = time.time() - start
    print('Training complete in {:.0f}m {:.0f}s'.format(
        time_elapsed // 60, time_elapsed % 60))
    print('Lowest Loss: {:4f}'.format(best_loss))

    # Save the best trained_models  weights
    model.load_state_dict(best_model_wts)
    torch.save(model.state_dict(),
               os.path.join(str(save_path), str(name_dataset) + '_run1_DeepLabv3plus.pt'))


# Main
def main(epochs, batch_size, num_classes, dir_report, num_channels, path, name_dataset):
    # Load images and Labels
    image_train, label_train = load_patches(path)
    data_train = SegmentationData(image_train, label_train)
    # Setting up the train dataset
    dataset_train = DataLoader(data_train, batch_size=batch_size, shuffle=True, num_workers=0)

    # Model call
    deeplabv3plus = deeplabv3plus_model(num_channel=num_channels, num_classes=num_classes)
    deeplabv3plus.train()

    # Print: summary
    summary(deeplabv3plus, (4, 256, 256))

    # Create the experiment directory if not present
    dir_report = Path(dir_report)
    if not dir_report.exists():
        dir_report.mkdir()

    # Specify the loss function
    criterion = torch.nn.CrossEntropyLoss()

    # Specify the optimizer with a lower learning rate
    optimizer = torch.optim.Adam(deeplabv3plus.parameters(), lr=0.0001)

    # Specify the evaluation metrics
    metrics = {'accuracy': accuracy_score, 'f1_score_weighted': f1_score,
               'balanced_accuracy': balanced_accuracy_score, 'iou': compute_iou}

    # Train
    train_model(model=deeplabv3plus, criterion=criterion,
                dataloaders=dataset_train,
                optimizer=optimizer, metrics=metrics, save_path=dir_report,
                num_epochs=epochs, name_dataset=name_dataset)

    # Inference

    torch.cuda.empty_cache()


# ------------- Applying -------------
cerradatav3 = '/Users/mateus.miranda/INPE-CAP/MSc/ai4luc/data/cerradatav3_1_splitted/train'

# Training CBERS4A RGB
if __name__ == '__main__':
    main(epochs=EPOCHS, batch_size=BATCH_SIZE, num_classes=NUM_CLASSES, path=cerradatav3,
         dir_report=exp_directory, num_channels=4, name_dataset='cerradatav3_semDA_2_')
